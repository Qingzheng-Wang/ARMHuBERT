teacher:
  teacher_model: ./hubert_base_ls960.pt

train:
  base_dir: ./
  output_dir: 'starhubert_base_ls960_100h_train_clean_100epc_mse'
  checkpoint: ./results/pretrain/starhubert_base_ls960_100h_kl_div/last.ckpt
  num_epochs: 200
  gpus: 1
  batch_size: 2
  accumulate_grad_batches: 12
  monitor_losses: true
  ##### TODO: Add these options in `train.py`
  fsp_type: both # feature space projection, [layer, intra, both]
  fsp_axis: channel # 'channel' or 'time', if channel, project on channel axis
  zeroth_input: pre_trf # if 'pre_trf', use pretrained feature extractor features
  fsp_loss_type: mse # 'mse' or 'l1' or 'KL'
  #####
  specaug: false
  use_fp16: false
  use_apex: false

distiller:
  # Extractor
  extractor_mode: default
  conv_feature_layers: '[(128, 10, 5)] + [(256, 1, 1)] + [(256, 3, 2)] * 4 + [(432,
    1, 1)] + [(432, 2, 2)] * 2'
  feature_grad_mult: 1.0

  # Convolutional relative positional encoding
  conv_bias: false
  conv_pos: 128
  conv_pos_groups: 16
  pos_conv_depth: 1
  max_positions: 100000

  # Transformer encoder
  layer_type: transformer
  encoder_layers: 12
  encoder_embed_dim: 432
  encoder_ffn_embed_dim: 976
  encoder_attention_heads: 8
  activation_fn: gelu
  layer_norm_first: false
  layer_grad_scale: false

  # Conformer
  # depthwise_conv_kernel_size: 31
  # attn_type: ''
  # pos_enc_type: abs
  # fp16: true

  # Dropout
  dropout: 0.1
  attention_dropout: 0.1
  activation_dropout: 0.1
  encoder_layerdrop: 0.0
  dropout_input: 0.05

  # Output
  final_dim: 768

  # Initialization
  init_conv_layers: false
  init_encoder_layers: 0

  # Time reduction layer
  # enable_tr_layer: false
  # tr_conv1d_kernel: 2
  # tr_layer_index: 0
  # tr_reduce_factor: 2
  # tr_layer_type: conv1d

  # Other
  checkpoint_activations: false
  required_seq_len_multiple: 1
  crop_seq_to_multiple: 1

optimizer:
  name: AdamW_with_schedule
  lr: 0.001
  schedule: warmup_cosine
  warmup_proportion: 0.05
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-06

data:
  bucketing_path: ./data/len_for_bucket
  libri_root: '/home/ubuntu/data/LibriSpeech'
  train_set: ['train-clean-100']
  test_set: ['test-clean']

specaug:
  adaptive: false
  adaptive_number_ratio: 0.04
  adaptive_size_ratio: 0.04
  max_n_time_masks: 20
  apply_time_warp: false
  apply_time_mask: true
  apply_freq_mask: true
  replace_with_zero: false
  time_warp_window: 5
  time_mask_width_range:
  - 0
  - 100
  freq_mask_width_range:
  - 0
  - 27
  num_freq_mask: 2
  num_time_mask: 2
